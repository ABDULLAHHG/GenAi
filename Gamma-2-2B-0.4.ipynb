{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21871465-9680-451d-b047-9a4416ecb387",
   "metadata": {},
   "source": [
    "### Gemma2 2B\n",
    "##### we will use gamma on the same task that we use llama3.1 8b notebook-0.4 , but \n",
    "*  No Quantization : that mean we will not lower the model precesion \n",
    "*  loading the model on float16 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5c5d5b-34a7-4387-b2fc-74f74e2be7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e5887-e076-48c0-814e-9858b56a6dea",
   "metadata": {},
   "source": [
    "### Create a simple dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7a261f-7030-4828-82be-54e0c71bef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"text\" :[\"Ahmed\" , \"Aboud\" , \"Mohammed\" , \"Moh\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36856467-256a-449b-be13-47e7cb4c91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(lambda x: f\"what is the first character in this name {x}. first character is  : {x[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ace34-6a37-42e5-8965-a9e53bb6e025",
   "metadata": {},
   "source": [
    "##### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22bc9901-00a8-4c5c-83ff-5d20125a3e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['what is the first character in this name Mohammed. first character is  : M'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(1).text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be20772d-fc72-4b8e-a6e6-9adb3d1d6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoModelForCausalLM, AutoTokenizer , BitsAndBytesConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d13e4-153f-4a99-aa98-cee1112151b5",
   "metadata": {},
   "source": [
    "#### Load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66f4c65-c166-460d-ab4b-6b69b24b448e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4ebdea88e344fdba6e6acb333738ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# choose the model\n",
    "model_id = \"google/gemma-2-2b\"\n",
    "\n",
    "# Load the model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=\"float16\",\n",
    "    # quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7c545-2c09-4af9-b400-d08b32659368",
   "metadata": {},
   "source": [
    "#### Load Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d442ebb0-ebb6-434c-8ab4-b2f84f14e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7b838-51a6-475c-8202-005c4829644d",
   "metadata": {},
   "source": [
    "#####  sfttrainer vs trainer\n",
    "* Use Trainer: If you have a large dataset and need extensive customization for your training loop or complex training workflows.\n",
    "* Use SFTTrainer: If you have a pre-trained model and a relatively smaller dataset, and want a simpler and faster fine-tuning experience with efficient memory usage.\n",
    "###### Source :[Difference between Trainer class and SFTTrainer (Supervised Fine tuning trainer) in Hugging Face?](https://medium.com/@sujathamudadla1213/difference-between-trainer-class-and-sfttrainer-supervised-fine-tuning-trainer-in-hugging-face-d295344d73f7#:~:text=Use%20Trainer%3A%20If%20you%20have%20a%20large%20dataset,and%20faster%20fine-tuning%20experience%20with%20efficient%20memory%20usage.)\n",
    "* it take 2 min to read "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6a35f-7380-4271-9376-3a1c455c4528",
   "metadata": {},
   "source": [
    "##### Convert it to  dataset \n",
    "###### [dataset documentation](https://huggingface.co/docs/datasets/v1.2.1/loading_datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3acbb05-cc5c-4a34-b9dd-804e8d236320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_data = Dataset.from_pandas(df[[\"text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc950db8-7a4e-4a42-9ecc-f2c4a64d994e",
   "metadata": {},
   "source": [
    "#### Lora Config \n",
    "###### [Lora Config Documentation](https://opendelta.readthedocs.io/en/latest/modules/deltas.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121673e6-2048-48e5-a972-b84ec80b6fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftConfig\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, #  A hyper-parameter to control the init scale of loralib.linear .\n",
    "    lora_dropout=0, # The dropout rate in lora.linear \n",
    "    r=64, # the rank of the lora parameters. The smaller lora_r is , the fewer parameters lora has.\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules = [\"q_proj\", \"k_proj\" , \"v_proj\" , \"o_proj\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97746f7-9919-44fe-848b-156974337573",
   "metadata": {},
   "source": [
    "#### TrainingArguments\n",
    "###### [TrainingArguments documentation](https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b16d7451-7500-4f58-a4ff-47922f40994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir=\"D:/llama-3.1-fine-tuned-model\" # Choose the directory u want to save the tmp in after saving its about 320MB \n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=5,                       # number of training epochs\n",
    "    logging_steps=1,  # to show training loss log    \n",
    "    \n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    \n",
    "    report_to=\"none\",                  #  to not report \n",
    "    eval_strategy=\"steps\",              # save checkpoint every epoch\n",
    "    eval_steps = 0.1 # to show val loss log                 \n",
    "    # 0.1 mean show eval log each epoch , 0.2 each 2 show one log etc \n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa14cd2-9b28-47f5-b0f1-5ab81c6d276b",
   "metadata": {},
   "source": [
    "#### Trainer by SFTTrainer\n",
    "###### [Supervised Fine-tuning Trainer documentation](https://huggingface.co/docs/trl/main/en/sft_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbd6454d-350d-410d-a3ee-618451006004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\envs\\pt\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "D:\\New folder\\envs\\pt\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "D:\\New folder\\envs\\pt\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969d1a29eb8b42fb9433456587d639cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2db66793114279941cf0ad60f3e249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\envs\\pt\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "D:\\New folder\\envs\\pt\\Lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=peft_config, # Lora config \n",
    "    args=training_arguments, # training argument \n",
    "    train_dataset=train_data, # dataset \n",
    "    tokenizer=tokenizer, # model tokeniaer \n",
    "    dataset_text_field=\"text\", # column name example df[[\"text\"]] u write text \n",
    "    eval_dataset=train_data, # evaluate dataset we will use the same dataset for now \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80717e6-2a80-4071-8e88-762301a3ae32",
   "metadata": {},
   "source": [
    "### train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "679fb696-48d0-44ed-bf8f-7272b5a4b339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n",
      "D:\\New folder\\envs\\pt\\Lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:520: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.307200</td>\n",
       "      <td>6.154595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.154600</td>\n",
       "      <td>6.013565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.013600</td>\n",
       "      <td>5.907961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.908000</td>\n",
       "      <td>5.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.833300</td>\n",
       "      <td>5.795701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=6.043327045440674, metrics={'train_runtime': 26.0461, 'train_samples_per_second': 0.768, 'train_steps_per_second': 0.192, 'total_flos': 4428166164480.0, 'train_loss': 6.043327045440674, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b43666-fe8a-4434-abf0-8bc22341b0d3",
   "metadata": {},
   "source": [
    "### Create test prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd15f17e-98b4-4577-8183-6394608368e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the first character in this name Ahmed. first character is  : A'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb485aee-aaa4-413d-aa8c-3b0ddf562b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"][0].split(\":\")[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06b6336d-07f1-45b4-a02c-e74f912839d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A : ['what is the first character in this name Ahmed. first character is  A.']\n",
      "A : ['what is the first character in this name Aboud. first character is  A']\n",
      "M : ['what is the first character in this name Mohammed. first character is  M.']\n",
      "M : ['what is the first character in this name Moh. first character is  M.']\n",
      "CPU times: total: 672 ms\n",
      "Wall time: 2.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# to show you how much time it take \n",
    "\n",
    "# choose a prompt \n",
    "for i in range(df.shape[0]):\n",
    "    prompt = df[\"text\"][i].split(\":\")[0]\n",
    "    \n",
    "    # convert it to tokenize it \n",
    "    token = tokenizer(prompt , return_tensors=\"pt\")\n",
    "    \n",
    "    # split it ot input_ids and attention_mask \n",
    "    input_ids = token.input_ids\n",
    "    attention_mask = token.attention_mask\n",
    "    \n",
    "    # use input to cuda \n",
    "    input_ids = input_ids.to('cuda')\n",
    "    attention_mask = attention_mask.to('cuda')\n",
    "    \n",
    "    \n",
    "    # then use the model to generate the output \n",
    "    outputs = model.generate(input_ids = input_ids , attention_mask = attention_mask  , max_length = 17 ,  pad_token_id = 128001  )\n",
    "    \n",
    "    # convert the output from tokens to text or readable language \n",
    "    print(df[\"text\"][i].split(\":\")[1][1] , \":\", tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920740e-9830-4213-9923-2cfbdc860ad3",
   "metadata": {},
   "source": [
    "* its good "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e8704-98a6-43b2-8aee-0d9ea3dc39b5",
   "metadata": {},
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5065f2c6-ef2b-4cb8-b86d-dd403ef2641d",
   "metadata": {},
   "source": [
    "* as we see its better than llama3.1 8B\n",
    "##### Reason\n",
    "* we have not lower the precesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723525b1-a9cb-47d7-a530-5fdf1be5029a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c3ebc-29cc-40da-a2df-8aebceca82fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
